{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chinedm/Alert-System-for-Safety-in-Taxi-Rides/blob/main/Drive_Alert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f847c4ec-f86a-4537-9d52-2afd619b1394",
      "metadata": {
        "id": "f847c4ec-f86a-4537-9d52-2afd619b1394"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import librosa\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nara_wpe"
      ],
      "metadata": {
        "id": "jgcOEvtOFuZd"
      },
      "id": "jgcOEvtOFuZd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d140754-2af2-44ba-b989-000003ebc899",
      "metadata": {
        "id": "3d140754-2af2-44ba-b989-000003ebc899"
      },
      "outputs": [],
      "source": [
        "from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration, AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1a7ecab-1e9a-498c-af87-480f4ef02bc3",
      "metadata": {
        "scrolled": true,
        "id": "e1a7ecab-1e9a-498c-af87-480f4ef02bc3",
        "outputId": "d23a2fe8-7b90-41ec-afbd-fb4109474252"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ifeoma_Amaechi\\anaconda3\\Lib\\site-packages\\pyannote\\audio\\core\\io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  torchaudio.set_audio_backend(\"soundfile\")\n"
          ]
        }
      ],
      "source": [
        "import pyannote.audio\n",
        "from datasets import load_dataset\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71aebdf6-ae12-4c0e-ae2c-c5c2affad6ba",
      "metadata": {
        "id": "71aebdf6-ae12-4c0e-ae2c-c5c2affad6ba"
      },
      "outputs": [],
      "source": [
        "!pip install pyannote.audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "423313a6-f928-4bcb-a149-c3db834716f1",
      "metadata": {
        "id": "423313a6-f928-4bcb-a149-c3db834716f1"
      },
      "outputs": [],
      "source": [
        "# instantiate the model\n",
        "from pyannote.audio import Model\n",
        "model = Model.from_pretrained(\n",
        "  \"pyannote/segmentation-3.0\",\n",
        "  use_auth_token=\"HUGGINGFACE_ACCESS_TOKEN_GOES_HERE\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cdfaf64-4eab-457d-9ee6-0ab67c200372",
      "metadata": {
        "id": "8cdfaf64-4eab-457d-9ee6-0ab67c200372"
      },
      "outputs": [],
      "source": [
        "pip install pyannote.audio --extra-index-url https://huggingface.co/pytorch/api/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"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "712c8e1b-2c02-491d-9530-749c33df5b07",
      "metadata": {
        "id": "712c8e1b-2c02-491d-9530-749c33df5b07",
        "outputId": "16096374-c2d7-4c0f-f15b-8019a353ea52"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Failed to import transformers.pipelines because of the following error (look up to see its traceback):\nDLL load failed while importing _pywrap_tf2: A dynamic link library (DLL) initialization routine failed.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1390\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1389\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1204\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap_external>:940\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:26\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedFeatureExtractor\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_processing_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseImageProcessor\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_auto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoConfig\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\image_processing_utils.py:28\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchFeature \u001b[38;5;28;01mas\u001b[39;00m BaseBatchFeature\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_transforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m center_crop, normalize, rescale\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChannelDimension\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\image_transforms.py:47\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tf_available():\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_flax_available():\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\__init__.py:42\u001b[0m\n\u001b[0;32m     41\u001b[0m _os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTF2_BEHAVIOR\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[0;32m     43\u001b[0m _tf2\u001b[38;5;241m.\u001b[39menable()\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\tf2.py:21\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Tools to help with the TensorFlow 2.0 transition.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mThis module is meant for TensorFlow internal implementation, not for users of\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03mthe TensorFlow library. For that see tf.compat instead.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pywrap_tf2\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_export\n",
            "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tf2: A dynamic link library (DLL) initialization routine failed.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnlpaug\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maugmenter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mword\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnaw\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nlpaug\\augmenter\\word\\__init__.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnlpaug\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maugmenter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mword\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtfidf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnlpaug\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maugmenter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mword\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspelling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnlpaug\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maugmenter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mword\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext_word_embs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnlpaug\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maugmenter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mword\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msynonym\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnlpaug\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maugmenter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mword\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mantonym\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nlpaug\\augmenter\\word\\context_word_embs.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnlpaug\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maugmenter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mword\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordAugmenter\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnlpaug\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlang_models\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnml\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnlpaug\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Action, Doc\n\u001b[0;32m     14\u001b[0m CONTEXT_WORD_EMBS_MODELS \u001b[38;5;241m=\u001b[39m {}\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nlpaug\\model\\lang_models\\__init__.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnlpaug\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlang_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mt5\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnlpaug\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlang_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbart\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnlpaug\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlang_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfill_mask_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnlpaug\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlang_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmachine_translation_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnlpaug\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlang_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummarization_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nlpaug\\model\\lang_models\\fill_mask_transformers.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# No installation required if not using this function\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1229\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1380\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1378\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1380\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m   1381\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1382\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1392\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1393\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1394\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1395\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.pipelines because of the following error (look up to see its traceback):\nDLL load failed while importing _pywrap_tf2: A dynamic link library (DLL) initialization routine failed."
          ]
        }
      ],
      "source": [
        "import nlpaug.augmenter.word as naw\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import smtplib\n",
        "from email.mime.text import MIMEText\n",
        "import requests\n",
        "import sqlite3\n",
        "from flask import Flask, render_template, request"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5263af04-f25f-408c-9baa-2e00423f8d8d",
      "metadata": {
        "id": "5263af04-f25f-408c-9baa-2e00423f8d8d"
      },
      "outputs": [],
      "source": [
        "!pip install nlpaug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bc86951-7ca6-4a44-8245-91177e5ab3f1",
      "metadata": {
        "id": "8bc86951-7ca6-4a44-8245-91177e5ab3f1"
      },
      "outputs": [],
      "source": [
        "# Data Preprocessing\n",
        "def remove_noise(audio_data, sampling_rate):\n",
        "    # Noise removal using Weighted Prediction Error (WPE)\n",
        "    wpe = nara_wpe.wpe.WPE(taps=10, delay=3, normalize_srft=True, statistics_mode=\"sample\")\n",
        "    enhanced_audio = wpe.run(audio_data, sampling_rate)\n",
        "    return enhanced_audio\n",
        "\n",
        "def diarize_speakers(audio_data, sampling_rate):\n",
        "    # Speaker diarization\n",
        "    speech_activity_detection = pyannote.audio.pipelines.SpeechActivityDetection()\n",
        "    speech_segments = speech_activity_detection(audio_data, sampling_rate)\n",
        "\n",
        "    speaker_diarization = pyannote.audio.pipelines.SpeakerDiarization()\n",
        "    speakers = speaker_diarization(audio_data, sampling_rate, speech_segments)\n",
        "\n",
        "    return speakers\n",
        "\n",
        "def transcribe_audio(audio_data, sampling_rate, model, processor):\n",
        "    # Automatic Speech Recognition (ASR)\n",
        "    transcription = model.transcribe(audio_data, sampling_rate=sampling_rate, language=\"en\", processor=processor)\n",
        "    return transcription\n",
        "\n",
        "def preprocess_audio(audio_data, sampling_rate):\n",
        "    cleaned_audio = remove_noise(audio_data, sampling_rate)\n",
        "    speakers = diarize_speakers(cleaned_audio, sampling_rate)\n",
        "    model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
        "    processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
        "    transcription = transcribe_audio(cleaned_audio, sampling_rate, model, processor)\n",
        "    return cleaned_audio, speakers, transcription"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e34601c-4241-4307-ad0c-f35f568a54e1",
      "metadata": {
        "id": "5e34601c-4241-4307-ad0c-f35f568a54e1"
      },
      "outputs": [],
      "source": [
        "# NLP Model Training\n",
        "def prepare_dataset():\n",
        "    # Prepare a labeled dataset\n",
        "    labels = {\"safe\": 0, \"threat\": 1, \"harassment\": 2, \"violence\": 3}\n",
        "    with open(\"taxi_safety_dataset.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow([\"text\", \"label\"])\n",
        "        writer.writerow([\"Can you please take me to the airport?\", labels[\"safe\"]])\n",
        "        writer.writerow([\"I'll hurt you if you don't give me your money.\", labels[\"threat\"]])\n",
        "        # Add more examples as needed\n",
        "\n",
        "    dataset = load_dataset(\"csv\", data_files=\"taxi_safety_dataset.csv\")\n",
        "    return dataset, labels\n",
        "\n",
        "def train_nlp_models(dataset, labels):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    tokenized_datasets = dataset.map(lambda examples: tokenizer(examples[\"text\"], truncation=True), batched=True)\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(labels))\n",
        "\n",
        "    training_args = TrainingArguments(output_dir=\"./results\", num_train_epochs=3, per_device_train_batch_size=16, per_device_eval_batch_size=16, warmup_steps=500, weight_decay=0.01, logging_dir=\"./logs\", logging_steps=10)\n",
        "\n",
        "    trainer = Trainer(model=model, args=training_args, train_dataset=tokenized_datasets[\"train\"], eval_dataset=tokenized_datasets[\"test\"])\n",
        "    trainer.train()\n",
        "\n",
        "    # Fine-tune the model for NER and sentiment analysis\n",
        "    # ... (code omitted for brevity)\n",
        "\n",
        "def augment_data(dataset):\n",
        "    augmentations = [naw.EasyDataAugmentation.synonym_replacement, naw.EasyDataAugmentation.random_swap, naw.EasyDataAugmentation.random_deletion]\n",
        "    augmented_dataset = []\n",
        "    for text, label in zip(dataset[\"text\"], dataset[\"label\"]):\n",
        "        augmented_texts = [text]\n",
        "        for aug in augmentations:\n",
        "            augmented_texts.extend(aug(text))\n",
        "\n",
        "        for augmented_text in augmented_texts:\n",
        "            augmented_dataset.append((augmented_text, label))\n",
        "\n",
        "    with open(\"augmented_taxi_safety_dataset.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow([\"text\", \"label\"])\n",
        "        for text, label in augmented_dataset:\n",
        "            writer.writerow([text, label])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef33522f-660f-4387-8608-45dc57a198a2",
      "metadata": {
        "id": "ef33522f-660f-4387-8608-45dc57a198a2"
      },
      "outputs": [],
      "source": [
        "# Real-time Inference\n",
        "def real_time_inference(audio_stream, asr_model, processor, nlp_models):\n",
        "    transcribed_text = transcribe_audio(audio_stream, sampling_rate, asr_model, processor)\n",
        "\n",
        "    # NLP processing\n",
        "    keywords, entities, sentiment = process_text(transcribed_text, nlp_models)\n",
        "    potential_threat = evaluate_threat(keywords, entities, sentiment)\n",
        "\n",
        "    if potential_threat:\n",
        "        generate_alert(potential_threat)\n",
        "        notify_contacts(potential_threat)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d6ec69e-a3e2-45f5-af71-e2ed5e76e49c",
      "metadata": {
        "id": "4d6ec69e-a3e2-45f5-af71-e2ed5e76e49c"
      },
      "outputs": [],
      "source": [
        "# Alert Generation and Notification\n",
        "def generate_alert(threat_level, details):\n",
        "    # Generate an alert based on the identified threat level and details\n",
        "    if threat_level == \"high\":\n",
        "        logger.critical(f\"High-level threat detected: {details}\")\n",
        "        # Trigger emergency response or escalation procedures\n",
        "    elif threat_level == \"medium\":\n",
        "        logger.warning(f\"Medium-level threat detected: {details}\")\n",
        "        # Notify designated contacts or monitoring station\n",
        "    elif threat_level == \"low\":\n",
        "        logger.info(f\"Low-level threat detected: {details}\")\n",
        "        # Log the incident for potential follow-up or analysis\n",
        "    else:\n",
        "        logger.error(f\"Invalid threat level: {threat_level}\")\n",
        "\n",
        "def send_email_alert(recipient, subject, body):\n",
        "    # Send an email alert to the specified recipient\n",
        "    msg = MIMEText(body)\n",
        "    msg[\"Subject\"] = subject\n",
        "    msg[\"From\"] = \"alerts@taxisafety.com\"\n",
        "    msg[\"To\"] = recipient\n",
        "\n",
        "    with smtplib.SMTP(\"smtp.gmail.com\", 587) as smtp:\n",
        "        smtp.starttls()\n",
        "        smtp.login(\"your_email@gmail.com\", \"your_password\")\n",
        "        smtp.send_message(msg)\n",
        "\n",
        "def send_sms_alert(phone_number, message):\n",
        "    # Send an SMS alert to the specified phone number\n",
        "    # Use a third-party SMS service API (e.g., Twilio, Plivo, etc.)\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19707a01-c818-4a16-88d8-6ad5e8574b31",
      "metadata": {
        "id": "19707a01-c818-4a16-88d8-6ad5e8574b31"
      },
      "outputs": [],
      "source": [
        "# Continuous Learning and Improvement\n",
        "def update_dataset(audio_stream, transcribed_text, potential_threat):\n",
        "    # Store incident data for continuous learning\n",
        "    pass\n",
        "\n",
        "def retrain_models():\n",
        "    # Periodically retrain and fine-tune the NLP models using the updated labeled data\n",
        "    pass\n",
        "\n",
        "# Deployment and Integration\n",
        "# Package the entire system as a containerized application or a microservice architecture\n",
        "# Integrate with taxi dispatch systems, location tracking, and other relevant services\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb6d3f36-3162-4a1f-8a8d-4f2d3f7ade34",
      "metadata": {
        "id": "eb6d3f36-3162-4a1f-8a8d-4f2d3f7ade34"
      },
      "outputs": [],
      "source": [
        "# User Interface or Dashboard\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Database setup\n",
        "conn = sqlite3.connect(\"alerts.db\")\n",
        "c = conn.cursor()\n",
        "c.execute('''CREATE TABLE IF NOT EXISTS alerts\n",
        "             (id INTEGER PRIMARY KEY AUTOINCREMENT, threat_level TEXT, details TEXT, timestamp TEXT)''')\n",
        "\n",
        "@app.route(\"/\")\n",
        "def index():\n",
        "    c.execute(\"SELECT * FROM alerts\")\n",
        "    alerts = c.fetchall()\n",
        "    return render_template(\"index.html\", alerts=alerts)\n",
        "\n",
        "@app.route(\"/create_alert\", methods=[\"POST\"])\n",
        "def create_alert():\n",
        "    threat_level = request.form[\"threat_level\"]\n",
        "    details = request.form[\"details\"]\n",
        "    timestamp = request.form[\"timestamp\"]\n",
        "    c.execute(\"INSERT INTO alerts (threat_level, details, timestamp) VALUES (?, ?, ?)\", (threat_level, details, timestamp))\n",
        "    conn.commit()\n",
        "    return \"Alert created successfully\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(debug=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1339e1ba-4ea0-433e-9104-f56327abfbb2",
      "metadata": {
        "id": "1339e1ba-4ea0-433e-9104-f56327abfbb2"
      },
      "outputs": [],
      "source": [
        "# RNN Models\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
        "        out, _ = self.gru(x, h0)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5cd494b-cca0-42f6-8eeb-e7a3cd709ff6",
      "metadata": {
        "id": "f5cd494b-cca0-42f6-8eeb-e7a3cd709ff6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d61787f0-6a0a-4ac1-a171-d16b4cac2ec1",
      "metadata": {
        "id": "d61787f0-6a0a-4ac1-a171-d16b4cac2ec1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}